{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"input\",\"context\"],\n",
    "    template='''\n",
    "\n",
    "[INST]\n",
    "\n",
    "You are a Virtual Assistant for students enrolled in the IITM BS Degree Program. Your primary role is to **answer queries related to the grading structure of any subject** in the program.  \n",
    "\n",
    "### Guidelines for Answering Grading-Related Queries:\n",
    "- The student will ask questions about the **grading structure**, including weightage of assignments, exams, quizzes, and overall evaluation criteria.\n",
    "- Use the **provided context** to give direct and accurate answers.\n",
    "- **DO NOT use the Socratic Method** or ask guiding questions for grading-related queries.\n",
    "- If **grading information is missing** from the context, politely inform the student that the details are unavailable and suggest checking official sources.\n",
    "\n",
    "---\n",
    "\n",
    "### Closing Statement\n",
    "**Always provide a clear, factual answer and invite further clarification:**  \n",
    "**“I hope this helps! Let me know if you need any more details. 😊”**  \n",
    "\n",
    "---\n",
    "\n",
    "**Student:** \"{input}\"  \n",
    "**Context to answer query:** \"{context}\"  \n",
    "\n",
    "[INST]\n",
    "\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asim/Documents/gitRepositories/soft-engg-project-jan-2025-se-Jan-21/backend/.venv/lib/python3.12/site-packages/langchain_together/llms.py:89: UserWarning: The completions endpoint, has 'max_tokens' as required argument. The default value is being set to 200 Consider setting this value, when initializing LLM\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_together import Together\n",
    "llm2 = Together(\n",
    "    temperature=0,\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Load the Grading document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pinecone vector DB\n",
    "\n",
    "from pinecone import Pinecone , ServerlessSpec\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name=\"test-rag\"\n",
    "\n",
    "pc_db= Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pc_db.has_index(index_name):\n",
    "    pc_db.create_index(\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader =   PyPDFLoader(\"assets/BS-grading_doc-Jan2025.pdf\")\n",
    "pages = loader.load()\n",
    "page = pages[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the pages\n",
    "from langchain.schema import Document\n",
    "\n",
    "merged_pages = \"/n\".join([page.page_content for page in pages])\n",
    "doc = Document(merged_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=126\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents([doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the TOGETHER_API_KEY in the .env file\n",
    "from langchain_together import TogetherEmbeddings\n",
    "embedding_model = TogetherEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = embedding_model.embed_query(\"What  are you dong\")\n",
    "len(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vector_store2 = Chroma.from_documents(split_docs,embedding_model , persist_directory=\"new_store2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "qa_chain = create_stuff_documents_chain(llm , template)\n",
    "RAG = create_retrieval_chain(vector_store2.as_retriever(), combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your Virtual Assistant for the IITM BS Degree Program. I'm here to help answer your queries related to the grading structure of any subject in the program. 😊\n"
     ]
    }
   ],
   "source": [
    "print(RAG.invoke({\"input\":\"Who are you?\"})[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = RAG.invoke({\"input\":\"What does final grade consist of in the software engineering course \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final grade in the software engineering course consists of several components: \n",
      "\n",
      "* 0.05GAA (Grade Point Average)\n",
      "* 0.2Qz2 (Quiz 2 score)\n",
      "* 0.4F (Final exam score)\n",
      "* 0.1GP1 (Group Project 1 score)\n",
      "* 0.1GP2 (Group Project 2 score)\n",
      "* 0.1PP (Peer Performance score)\n",
      "* 0.05CP (Class Participation score)\n",
      "* 0.4max(Qz1,Qz2) (Maximum of Quiz 1 and Quiz 2 scores)\n",
      "\n",
      "I hope this helps! Let me know if you need any more details. 😊\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the final grade in the software testing course consists of:\n",
      "\n",
      "* 0.4max(Qz1,Qz2) (quiz scores)\n",
      "* End Term exam (in-person, 30 marks)\n",
      "\n",
      "Note that the W12 assignment score is not included in the final grade, but its contents will be included in the final exam.\n",
      "\n",
      "I hope this helps! Let me know if you need any more details. 😊\n"
     ]
    }
   ],
   "source": [
    "print(RAG.invoke({\"input\":\"What does final grade consist of in the software testing course\"})[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "qa_chain2 = create_stuff_documents_chain(llm2 , template)\n",
    "RAG2 = create_retrieval_chain(vector_store2.as_retriever(), combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the final grade in the software testing course consists of:\n",
      "\n",
      "* 0.4max(Qz1,Qz2) (quiz scores)\n",
      "* End Term exam (in-person, 30 marks)\n",
      "\n",
      "Note that the W12 assignment score is not included in the final grade, but its contents will be included in the final exam.\n",
      "\n",
      "I hope this helps! Let me know if you need any more details. 😊\n"
     ]
    }
   ],
   "source": [
    "print(RAG2.invoke({\"input\":\"What does final grade consist of in the software testing course\"})[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
